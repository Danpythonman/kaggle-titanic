{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Titanic Kaggle Competition\n",
    "\n",
    "The Titanic competition on Kaggle presents the challenge of identifying the factors that contribute to surviving the sinking of the ship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import typing\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.figure import Figure\n",
    "from matplotlib.axes import Axes\n",
    "PltAxes: typing.TypeAlias = typing.Union[typing.Sequence[Axes], typing.Sequence[typing.Sequence[Axes]], np.ndarray, Axes]\n",
    "\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "We have the following notes about the dataset:\n",
    "- `survival` Survival (0 = No, 1 = Yes)\n",
    "- `pclass` Ticket class (1 = 1st, 2 = 2nd, 3 = 3rd)\n",
    "- `sex` Sex\n",
    "- `Age` Age in years\n",
    "- `sibsp` # of siblings / spouses aboard the Titanic\n",
    "- `parch` # of parents / children aboard the Titanic\n",
    "- `ticket` Ticket number\n",
    "- `fare` Passenger fare\n",
    "- `cabin` Cabin number\n",
    "- `embarked` Port of Embarkation (C = Cherbourg, Q = Queenstown, S = Southampton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "It's also important to see how much data is missing so we can figure out the best way to handle it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Exploration\n",
    "\n",
    "Here we wil visualize different aspects of the data to find promising features that can help us predict survival."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_survived = len(df[df['Survived'] == 1])\n",
    "count_died = len(df[df['Survived'] == 0])\n",
    "count_all = len(df)\n",
    "plt.figure()\n",
    "plt.pie([count_survived, count_died], colors=['tab:green', 'tab:red'], labels=[f'Survived: {count_survived} ({100*count_survived/count_all:.2f}%)', f'Died: {count_died} ({100*count_died/count_all:.2f}%)'])\n",
    "plt.title('Total number of passengers that survived and died')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram(df: pd.DataFrame, feature: str, axes: Axes, title: typing.Optional[str] = None, xlabel: typing.Optional[str] = None, ylabel: typing.Optional[str] = None, dropna: bool = False) -> None:\n",
    "    '''Plot stacked histogram of amount of passengers that survived/died.'''\n",
    "    survived = df[df['Survived'] == 1][feature]\n",
    "    died = df[df['Survived'] == 0][feature]\n",
    "    if dropna:\n",
    "        survived = survived.dropna()\n",
    "        died = died.dropna()\n",
    "    axes.hist([survived, died], stacked=True, color=['tab:green', 'tab:red'], label=['Survived', 'Died'])\n",
    "    axes.legend()\n",
    "    axes.set_xlabel(xlabel if xlabel else feature)\n",
    "    axes.set_ylabel(ylabel if ylabel else 'Amount')\n",
    "    axes.set_title(title if title else feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "Some obvious (and easily analyzable) features to check are fare, passenger class, sex, age, number of siblings, number of parents, and embarked location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig: Figure\n",
    "axes: PltAxes\n",
    "features = ['Fare', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Embarked']\n",
    "fig, axes = plt.subplots(1, len(features), figsize=(35, 5))\n",
    "for i, feature in enumerate(features):\n",
    "    histogram(df, feature, axes[i], dropna=True)\n",
    "fig.suptitle('Amount of passengers survived/died for feature:')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "There are a few easily observable heuristics that seem to be generally true. For example, male, 3rd class, and low-fare passengers were less likely to survive. However, there are not any glaringly obvious survival indicators we can notice by analyzing any single attribute.\n",
    "\n",
    "This indicates that if there is a way to predict survival, it will be a mix of these features.\n",
    "\n",
    "Before conducting a more intense analysis, we are going to explore the features we left out: name, ticket, and cabin.\n",
    "\n",
    "Let's start with cabin. Upon first glance, we notice that most of the values for cabin are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df['Cabin'].isna())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "Let's check if the absence of a cabin attribute affects the survival of a passenger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cabin_na(x: float | str):\n",
    "    if pd.isna(x):\n",
    "        return 'Cabin is N/A'\n",
    "    return 'Cabin is not N/A'\n",
    "df['CabinNa'] = df['Cabin'].apply(cabin_na)\n",
    "plt.figure()\n",
    "histogram(df, 'CabinNa', plt.gca(), 'Passenger survival and missing cabin datapoint', 'Cabin datapoint presence', 'Amount of passengers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "Finding the unique values of cabin, we see that they are alphanumeric strings. The letter at the beginning seems to correspond to a deck of the ship ([https://www.encyclopedia-titanica.org/titanic-deckplans/](https://www.encyclopedia-titanica.org/titanic-deckplans/)), which could influence survival probability (rooms closer to the iceberg would be more affected, rooms deeper in the ship would have a farther distance to the life boats). Given that the crash occurred late in the night ([https://www.thoughtco.com/titanic-timeline-1779210](https://www.thoughtco.com/titanic-timeline-1779210)), it is likely that many people would be in their rooms when the Titanic hit the iceberg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Cabin'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cabin_start_char(x: float | str):\n",
    "    if pd.isna(x):\n",
    "        return x\n",
    "    return x[0]\n",
    "df['CabinStartChar'] = df['Cabin'].apply(cabin_start_char)\n",
    "plt.figure()\n",
    "histogram(df, 'CabinStartChar', plt.gca(), 'Passenger survival and first letter of cabin', 'Starting letter of cabin', 'Amount of passengers', dropna=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "For now we will skip the ticket and name features. The ticket likely either random or corresponds to the passenger class, fare, and location of purchase. The name would require complex processing and NLP, so for simplicity we ignore it for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## Preparing Data\n",
    "\n",
    "Now that we know what features we will be working with, we can clean up the data to be processed more easily. We will likely be using a neural network, so we want numerical columns to be normalized and categorical columns one-hot encoded.\n",
    "\n",
    "First, note that fare has a high variance and is very long-tailed (very few tickets are very expensive, most are much cheaper)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(df['Fare'], bins=50)\n",
    "plt.title('Fare histogram')\n",
    "plt.xlabel('Fare ($)')\n",
    "plt.ylabel('Number of tickets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "A good way to normalize this column would be to take the log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['FareLog'] = np.log1p(df['Fare'])\n",
    "plt.figure()\n",
    "plt.hist(df['FareLog'], bins=50)\n",
    "plt.title('Logarithm of fare histogram')\n",
    "plt.xlabel('Log(Fare) ($)')\n",
    "plt.ylabel('Number of tickets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "We are also going to Z-score normalize the log-normalized data because neural networks like when data has mean 0 and std 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['FareLogZ'] = (df['FareLog'] - df['FareLog'].mean()) / df['FareLog'].std()\n",
    "plt.figure()\n",
    "plt.hist(df['FareLogZ'], bins=50)\n",
    "plt.title('Z-Score normalized logarithm of fare histogram')\n",
    "plt.xlabel('Z-Score normalized Log(Fare) ($)')\n",
    "plt.ylabel('Number of tickets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "The other numerical category is age, which already looks approximately normally distributed. I think we can get away with just Z-score normalization here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['AgeZ'] = (df['Age'] - df['Age'].mean()) / df['Age'].std()\n",
    "fig: Figure\n",
    "axes: PltAxes\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "axes[0].hist(df['Age'], bins=50)\n",
    "axes[0].set_title('Age Histogram')\n",
    "axes[0].set_xlabel('Age')\n",
    "axes[0].set_ylabel('Number of passengers')\n",
    "axes[1].hist(df['AgeZ'], bins=50)\n",
    "axes[1].set_title('Z-score normalized age histogram')\n",
    "axes[1].set_xlabel('Z-score normalized age')\n",
    "axes[1].set_ylabel('Number of passengers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "The rest of the data is categorical. To turn this into neural-network friendly inputs we will use one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, columns=['Pclass', 'Sex', 'SibSp', 'Parch', 'Embarked', 'CabinStartChar'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "We now have a 32-dimensional input vector and a 1-dimension output. We  have 891 data points in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in = df.drop(columns=['PassengerId', 'Name', 'Survived', 'Age', 'Ticket', 'Cabin', 'CabinNa', 'Fare', 'FareLog', 'Age'])\n",
    "df_out = df[['Survived']]\n",
    "print(f'{df_in.shape=}, {df_out.shape=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "To make this data cleaning process repeatable, we will collect the steps into a function so that we can perform it on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_clean = df.copy()\n",
    "\n",
    "    df_clean = df_clean.drop(columns=['PassengerId', 'Name', 'Ticket'])\n",
    "\n",
    "    df_clean['CabinStartChar'] = df_clean['Cabin'].apply(cabin_start_char)\n",
    "    df_clean = df_clean.drop(columns=['Cabin'])\n",
    "\n",
    "    df_clean['FareLog'] = np.log1p(df_clean['Fare'])\n",
    "    df_clean['FareLogZ'] = (df_clean['FareLog'] - df_clean['FareLog'].mean()) / df_clean['FareLog'].std()\n",
    "    df_clean = df_clean.drop(columns=['Fare', 'FareLog'])\n",
    "\n",
    "    df_clean['Age'] = df_clean['Age'].fillna(df_clean['Age'].mean())\n",
    "    df_clean['AgeZ'] = (df_clean['Age'] - df_clean['Age'].mean()) / df_clean['Age'].std()\n",
    "    df_clean = df_clean.drop(columns=['Age'])\n",
    "\n",
    "    df_clean = pd.get_dummies(df_clean, columns=['Pclass', 'Sex', 'SibSp', 'Parch', 'Embarked', 'CabinStartChar'])\n",
    "\n",
    "    df_clean = df_clean.astype(float)\n",
    "\n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "We also define a dataset class to work well with PyTorch's data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PandasDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, df: pd.DataFrame, features: typing.Sequence[str], label: str, dtype: typing.Optional[torch.dtype] = torch.float32, device: typing.Optional[torch.device] = None):\n",
    "        self.X = torch.tensor(df[features].values, dtype=dtype, device=device)\n",
    "        self.y = torch.tensor(df[label].values, dtype=dtype, device=device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "Now we load the data again, clean it, and split it into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "df_clean = clean_data(df)\n",
    "df_train, df_val = train_test_split(df_clean, test_size=0.2, random_state=42, stratify=df['Survived'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "These are the features we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 'Survived'\n",
    "features = [feature for feature in df_clean.columns.tolist() if feature != label]\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "## Defining the Neural Network\n",
    "\n",
    "We have a 32-dimensional input vector, so the neural network will start with 33 inputs. We will treat this as a binary classification problem where we predict 1 (survived) or 0 (died)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    collections.OrderedDict([\n",
    "        ('lin1', nn.Linear(32, 48)),\n",
    "        ('norm1', nn.BatchNorm1d(48)),\n",
    "        ('gelu1', nn.GELU()),\n",
    "        ('drop1', nn.Dropout(0.5)),\n",
    "\n",
    "        ('lin2', nn.Linear(48, 64)),\n",
    "        ('norm2', nn.BatchNorm1d(64)),\n",
    "        ('gelu2', nn.GELU()),\n",
    "        ('drop2', nn.Dropout(0.5)),\n",
    "\n",
    "        ('lin3', nn.Linear(64, 96)),\n",
    "        ('norm3', nn.BatchNorm1d(96)),\n",
    "        ('gelu3', nn.GELU()),\n",
    "        ('drop3', nn.Dropout(0.5)),\n",
    "\n",
    "        ('lin4', nn.Linear(96, 1))\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "Hopefully we have a GPU so that training is faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "We turn our training and validation datasets into data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PandasDataset(df_train, features, label, device=device)\n",
    "val_dataset = PandasDataset(df_val, features, label, device=device)\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "We prepare for the training loop by moving the model to the device we're using (hopefully a GPU), define the optimizer used to update our weights and biases (AdamW), and specify that our loss function is binary cross entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "We also include a function to calculate the loss and accuracy over an entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def loss_and_accuracy(split: str):\n",
    "    X: Tensor\n",
    "    y: Tensor\n",
    "    loss: Tensor\n",
    "    logits: Tensor\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total = 0\n",
    "    if split == 'train':\n",
    "        data_loader = train_loader\n",
    "    elif split == 'val':\n",
    "        data_loader = val_loader\n",
    "    else:\n",
    "        raise Exception(f'Invalid split {split}')\n",
    "    for X, y in data_loader:\n",
    "        logits = model(X)\n",
    "        loss = criterion(logits.squeeze(), y)\n",
    "        predictions = (torch.sigmoid(logits.squeeze()) >= 0.5).long()\n",
    "        total_correct += (predictions == y).sum().item()\n",
    "        total += y.shape[0]\n",
    "        total_loss += loss.item() * y.shape[0]\n",
    "    avg_loss = total_loss / total\n",
    "    accuracy = total_correct / total\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "This training loop trains the model with back-propagation and returns details about the training process (like loss and accuracy throughout the training run)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: nn.Module, epochs: int):\n",
    "    train_val_details = []\n",
    "    losses = []\n",
    "    learning_rates = []\n",
    "    for epoch in tqdm.tqdm(range(epochs)):\n",
    "        model.eval()\n",
    "        train_loss, train_accuracy = loss_and_accuracy('train')\n",
    "        val_loss, val_accuracy = loss_and_accuracy('val')\n",
    "        train_val_details.append((train_loss, train_accuracy, val_loss, val_accuracy))\n",
    "        learning_rates.append(scheduler.get_last_lr()[0])\n",
    "        model.train()\n",
    "        X: Tensor\n",
    "        y: Tensor\n",
    "        loss: Tensor\n",
    "        for X, y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(X)\n",
    "            loss = criterion(logits.squeeze(), y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "        scheduler.step()\n",
    "        epoch += 1\n",
    "        if epoch >= epochs:\n",
    "            break\n",
    "    df_train_val = pd.DataFrame(train_val_details, columns=['train loss', 'train accuracy', 'val loss', 'val accuracy'])\n",
    "    losses = np.array(losses)\n",
    "    learning_rates = np.array(learning_rates)\n",
    "    return df_train_val, losses, learning_rates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "We also define this plotting function to plot the details of the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_plot(df_training_details: pd.DataFrame, losses: np.ndarray, learning_rates: np.ndarray, axes: PltAxes):\n",
    "    axes[0].scatter(np.arange(len(losses)), losses, alpha=0.5)\n",
    "    axes[0].set_title('Loss over training')\n",
    "    axes[0].set_xlabel('Iteration')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].grid(True)\n",
    "\n",
    "    axes[1].scatter(np.arange(df_training_details['train loss'].shape[0]), df_training_details['train loss'], label='training loss', alpha=0.5)\n",
    "    axes[1].scatter(np.arange(df_training_details['val loss'].shape[0]), df_training_details['val loss'], label='validation loss', alpha=0.5)\n",
    "    axes[1].set_title('Training and validation loss over training')\n",
    "    axes[1].set_xlabel('Iteration')\n",
    "    axes[1].set_ylabel('Loss')\n",
    "    axes[1].grid(True)\n",
    "    axes[1].legend()\n",
    "\n",
    "    axes[2].scatter(np.arange(df_training_details['train accuracy'].shape[0]), df_training_details['train accuracy'], label='training accuracy', alpha=0.5)\n",
    "    axes[2].scatter(np.arange(df_training_details['val accuracy'].shape[0]), df_training_details['val accuracy'], label='validation accuracy', alpha=0.5)\n",
    "    axes[2].set_title('Training and validation accuracy over training')\n",
    "    axes[2].set_xlabel('Iteration')\n",
    "    axes[2].set_ylabel('Accuracy')\n",
    "    axes[2].grid(True)\n",
    "    axes[2].legend()\n",
    "\n",
    "    axes[3].scatter(np.arange(df_training_details['train accuracy'].shape[0]), df_training_details['train accuracy'], label='training accuracy', alpha=0.5)\n",
    "    axes[3].scatter(np.arange(df_training_details['val accuracy'].shape[0]), df_training_details['val accuracy'], label='validation accuracy', alpha=0.5)\n",
    "    axes[3].set_title('Training and validation accuracy over training\\nwith y-axis from 0 to 1')\n",
    "    axes[3].set_xlabel('Iteration'); axes[3].set_ylabel('Accuracy')\n",
    "    axes[3].grid(True)\n",
    "    axes[3].legend()\n",
    "    axes[3].set_ylim([0, 1])\n",
    "\n",
    "    axes[4].scatter(np.arange(len(learning_rates)), learning_rates, alpha=0.5)\n",
    "    axes[4].set_title('Learning rate over training')\n",
    "    axes[4].set_xlabel('Epoch')\n",
    "    axes[4].set_ylabel('Learning rate')\n",
    "    axes[4].grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "Here we train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training_details, losses, learning_rates = train(model, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig: Figure\n",
    "axes: PltAxes\n",
    "fig, axes = plt.subplots(1, 5, figsize=(35, 5))\n",
    "training_plot(df_training_details, losses, learning_rates, axes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "### Using a Larger Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    collections.OrderedDict([\n",
    "        ('lin1', nn.Linear(32, 48)),\n",
    "        ('norm1', nn.BatchNorm1d(48)),\n",
    "        ('gelu1', nn.GELU()),\n",
    "        ('drop1', nn.Dropout(0.5)),\n",
    "\n",
    "        ('lin2', nn.Linear(48, 64)),\n",
    "        ('norm2', nn.BatchNorm1d(64)),\n",
    "        ('gelu2', nn.GELU()),\n",
    "        ('drop2', nn.Dropout(0.5)),\n",
    "\n",
    "        ('lin3', nn.Linear(64, 96)),\n",
    "        ('norm3', nn.BatchNorm1d(96)),\n",
    "        ('gelu3', nn.GELU()),\n",
    "        ('drop3', nn.Dropout(0.5)),\n",
    "\n",
    "        ('lin4', nn.Linear(96, 128)),\n",
    "        ('norm4', nn.BatchNorm1d(128)),\n",
    "        ('gelu4', nn.GELU()),\n",
    "        ('drop4', nn.Dropout(0.5)),\n",
    "\n",
    "        ('lin5', nn.Linear(128, 256)),\n",
    "        ('norm5', nn.BatchNorm1d(256)),\n",
    "        ('gelu5', nn.GELU()),\n",
    "        ('drop5', nn.Dropout(0.5)),\n",
    "\n",
    "        ('lin6', nn.Linear(256, 512)),\n",
    "        ('norm6', nn.BatchNorm1d(512)),\n",
    "        ('gelu6', nn.GELU()),\n",
    "        ('drop6', nn.Dropout(0.5)),\n",
    "\n",
    "        ('lin7', nn.Linear(512, 128)),\n",
    "        ('norm7', nn.BatchNorm1d(128)),\n",
    "        ('gelu7', nn.GELU()),\n",
    "        ('drop7', nn.Dropout(0.5)),\n",
    "\n",
    "        ('lin8', nn.Linear(128, 1))\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training_details, losses, learning_rates = train(model, epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig: Figure\n",
    "axes: PltAxes\n",
    "fig, axes = plt.subplots(1, 5, figsize=(35, 5))\n",
    "training_plot(df_training_details, losses, learning_rates, axes)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_cuda12.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
