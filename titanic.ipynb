{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cf00a2b",
   "metadata": {},
   "source": [
    "# Titanic Kaggle Competition\n",
    "\n",
    "The Titanic competition on Kaggle presents the challenge of identifying the factors that contribute to surviving the sinking of the ship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04109da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import typing\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.figure import Figure\n",
    "from matplotlib.axes import Axes\n",
    "PltAxes: typing.TypeAlias = typing.Union[typing.Sequence[Axes], typing.Sequence[typing.Sequence[Axes]], np.ndarray, Axes]\n",
    "\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547d76aa",
   "metadata": {},
   "source": [
    "We have the following notes about the dataset:\n",
    "- `survival` Survival (0 = No, 1 = Yes)\n",
    "- `pclass` Ticket class (1 = 1st, 2 = 2nd, 3 = 3rd)\n",
    "- `sex` Sex\n",
    "- `Age` Age in years\n",
    "- `sibsp` # of siblings / spouses aboard the Titanic\n",
    "- `parch` # of parents / children aboard the Titanic\n",
    "- `ticket` Ticket number\n",
    "- `fare` Passenger fare\n",
    "- `cabin` Cabin number\n",
    "- `embarked` Port of Embarkation (C = Cherbourg, Q = Queenstown, S = Southampton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5272d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0d3318",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe143533",
   "metadata": {},
   "source": [
    "It's also important to see how much data is missing so we can figure out the best way to handle it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fd7bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a6e837",
   "metadata": {},
   "source": [
    "## Exploration\n",
    "\n",
    "Here we wil visualize different aspects of the data to find promising features that can help us predict survival."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d26a66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_survived = len(df[df['Survived'] == 1])\n",
    "count_died = len(df[df['Survived'] == 0])\n",
    "count_all = len(df)\n",
    "plt.figure()\n",
    "plt.pie([count_survived, count_died], colors=['tab:green', 'tab:red'], labels=[f'Survived: {count_survived} ({100*count_survived/count_all:.2f}%)', f'Died: {count_died} ({100*count_died/count_all:.2f}%)'])\n",
    "plt.title('Total number of passengers that survived and died')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02810632",
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram(df: pd.DataFrame, feature: str, axes: Axes, title: typing.Optional[str] = None, xlabel: typing.Optional[str] = None, ylabel: typing.Optional[str] = None, dropna: bool = False) -> None:\n",
    "    '''Plot stacked histogram of amount of passengers that survived/died.'''\n",
    "    survived = df[df['Survived'] == 1][feature]\n",
    "    died = df[df['Survived'] == 0][feature]\n",
    "    if dropna:\n",
    "        survived = survived.dropna()\n",
    "        died = died.dropna()\n",
    "    axes.hist([survived, died], stacked=True, color=['tab:green', 'tab:red'], label=['Survived', 'Died'])\n",
    "    axes.legend()\n",
    "    axes.set_xlabel(xlabel if xlabel else feature)\n",
    "    axes.set_ylabel(ylabel if ylabel else 'Amount')\n",
    "    axes.set_title(title if title else feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05d0304",
   "metadata": {},
   "source": [
    "Some obvious (and easily analyzable) features to check are fare, passenger class, sex, age, number of siblings, number of parents, and embarked location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8857cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig: Figure\n",
    "axes: PltAxes\n",
    "features = ['Fare', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Embarked']\n",
    "fig, axes = plt.subplots(1, len(features), figsize=(35, 5))\n",
    "for i, feature in enumerate(features):\n",
    "    histogram(df, feature, axes[i], dropna=True)\n",
    "fig.suptitle('Amount of passengers survived/died for feature:')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b462c4",
   "metadata": {},
   "source": [
    "There are a few easily observable heuristics that seem to be generally true. For example, male, 3rd class, and low-fare passengers were less likely to survive. However, there are not any glaringly obvious survival indicators we can notice by analyzing any single attribute.\n",
    "\n",
    "This indicates that if there is a way to predict survival, it will be a mix of these features.\n",
    "\n",
    "Before conducting a more intense analysis, we are going to explore the features we left out: name, ticket, and cabin.\n",
    "\n",
    "Let's start with cabin. Upon first glance, we notice that most of the values for cabin are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52961e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df['Cabin'].isna())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06971a04",
   "metadata": {},
   "source": [
    "Let's check if the absence of a cabin attribute affects the survival of a passenger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1069d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cabin_na(x: float | str):\n",
    "    if pd.isna(x):\n",
    "        return 'Cabin is N/A'\n",
    "    return 'Cabin is not N/A'\n",
    "df['CabinNa'] = df['Cabin'].apply(cabin_na)\n",
    "plt.figure()\n",
    "histogram(df, 'CabinNa', plt.gca(), 'Passenger survival and missing cabin datapoint', 'Cabin datapoint presence', 'Amount of passengers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b74b2cd",
   "metadata": {},
   "source": [
    "Finding the unique values of cabin, we see that they are alphanumeric strings. The letter at the beginning seems to correspond to a deck of the ship ([https://www.encyclopedia-titanica.org/titanic-deckplans/](https://www.encyclopedia-titanica.org/titanic-deckplans/)), which could influence survival probability (rooms closer to the iceberg would be more affected, rooms deeper in the ship would have a farther distance to the life boats). Given that the crash occurred late in the night ([https://www.thoughtco.com/titanic-timeline-1779210](https://www.thoughtco.com/titanic-timeline-1779210)), it is likely that many people would be in their rooms when the Titanic hit the iceberg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3069179",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Cabin'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4c04d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cabin_start_char(x: float | str):\n",
    "    if pd.isna(x):\n",
    "        return x\n",
    "    return x[0]\n",
    "df['CabinStartChar'] = df['Cabin'].apply(cabin_start_char)\n",
    "plt.figure()\n",
    "histogram(df, 'CabinStartChar', plt.gca(), 'Passenger survival and first letter of cabin', 'Starting letter of cabin', 'Amount of passengers', dropna=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59af56a0",
   "metadata": {},
   "source": [
    "For now we will skip the ticket and name features. The ticket likely either random or corresponds to the passenger class, fare, and location of purchase. The name would require complex processing and NLP, so for simplicity we ignore it for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1e36ec",
   "metadata": {},
   "source": [
    "## Preparing Data\n",
    "\n",
    "Now that we know what features we will be working with, we can clean up the data to be processed more easily. We will likely be using a neural network, so we want numerical columns to be normalized and categorical columns one-hot encoded.\n",
    "\n",
    "First, note that fare has a high variance and is very long-tailed (very few tickets are very expensive, most are much cheaper)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773fda65",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(df['Fare'], bins=50)\n",
    "plt.title('Fare histogram')\n",
    "plt.xlabel('Fare ($)')\n",
    "plt.ylabel('Number of tickets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece171bf",
   "metadata": {},
   "source": [
    "A good way to normalize this column would be to take the log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c0bdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['FareLog'] = np.log1p(df['Fare'])\n",
    "plt.figure()\n",
    "plt.hist(df['FareLog'], bins=50)\n",
    "plt.title('Logarithm of fare histogram')\n",
    "plt.xlabel('Log(Fare) ($)')\n",
    "plt.ylabel('Number of tickets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56773c78",
   "metadata": {},
   "source": [
    "We are also going to Z-score normalize the log-normalized data because neural networks like when data has mean 0 and std 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee04669f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['FareLogZ'] = (df['FareLog'] - df['FareLog'].mean()) / df['FareLog'].std()\n",
    "plt.figure()\n",
    "plt.hist(df['FareLogZ'], bins=50)\n",
    "plt.title('Z-Score normalized logarithm of fare histogram')\n",
    "plt.xlabel('Z-Score normalized Log(Fare) ($)')\n",
    "plt.ylabel('Number of tickets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c0b050",
   "metadata": {},
   "source": [
    "The other numerical category is age, which already looks approximately normally distributed. I think we can get away with just Z-score normalization here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb27fca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['AgeZ'] = (df['Age'] - df['Age'].mean()) / df['Age'].std()\n",
    "fig: Figure\n",
    "axes: PltAxes\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "axes[0].hist(df['Age'], bins=50)\n",
    "axes[0].set_title('Age Histogram')\n",
    "axes[0].set_xlabel('Age')\n",
    "axes[0].set_ylabel('Number of passengers')\n",
    "axes[1].hist(df['AgeZ'], bins=50)\n",
    "axes[1].set_title('Z-score normalized age histogram')\n",
    "axes[1].set_xlabel('Z-score normalized age')\n",
    "axes[1].set_ylabel('Number of passengers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab89751",
   "metadata": {},
   "source": [
    "The rest of the data is categorical. To turn this into neural-network friendly inputs we will use one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82dfde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, columns=['Pclass', 'Sex', 'SibSp', 'Parch', 'Embarked', 'CabinStartChar'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c700f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77622c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c51a868",
   "metadata": {},
   "source": [
    "We now have a 32-dimensional input vector and a 1-dimension output. We  have 891 data points in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28b1eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in = df.drop(columns=['PassengerId', 'Name', 'Survived', 'Age', 'Ticket', 'Cabin', 'CabinNa', 'Fare', 'FareLog', 'Age'])\n",
    "df_out = df[['Survived']]\n",
    "print(f'{df_in.shape=}, {df_out.shape=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ac975f",
   "metadata": {},
   "source": [
    "To make this data cleaning process repeatable, we will collect the steps into a function so that we can perform it on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16e3928",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df: pd.DataFrame, drop_columns: bool = True, keep_id: bool = False) -> pd.DataFrame:\n",
    "    df_clean = df.copy()\n",
    "\n",
    "    df_clean['CabinStartChar'] = df_clean['Cabin'].apply(cabin_start_char)\n",
    "\n",
    "    df_clean['FareLog'] = np.log1p(df_clean['Fare'])\n",
    "    df_clean['FareLogZ'] = (df_clean['FareLog'] - df_clean['FareLog'].mean()) / df_clean['FareLog'].std()\n",
    "\n",
    "    df_clean['Age'] = df_clean['Age'].fillna(df_clean['Age'].mean())\n",
    "    df_clean['AgeZ'] = (df_clean['Age'] - df_clean['Age'].mean()) / df_clean['Age'].std()\n",
    "\n",
    "    non_dummy_columns = df_clean.columns\n",
    "\n",
    "    df_clean = pd.get_dummies(df_clean, columns=['Pclass', 'Sex', 'SibSp', 'Parch', 'Embarked', 'CabinStartChar'])\n",
    "\n",
    "    dummy_columns = df_clean.columns.difference(non_dummy_columns)\n",
    "    feature_columns = ['FareLogZ', 'AgeZ'] + list(dummy_columns)\n",
    "\n",
    "    if 'Survived' in df_clean.columns:\n",
    "        feature_columns += ['Survived']\n",
    "\n",
    "    df_clean[feature_columns] = df_clean[feature_columns].astype(float)\n",
    "    if drop_columns:\n",
    "        if keep_id:\n",
    "            df_clean = df_clean[['PassengerId'] + feature_columns]\n",
    "        else:\n",
    "            df_clean = df_clean[feature_columns]\n",
    "    else:\n",
    "        if not keep_id:\n",
    "            df_clean.drop(columns=['PassengerId'])\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44453608",
   "metadata": {},
   "source": [
    "We also define a dataset class to work well with PyTorch's data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef04ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PandasDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, df: pd.DataFrame, features: typing.Sequence[str], label: str, dtype: typing.Optional[torch.dtype] = torch.float32, device: typing.Optional[torch.device] = None):\n",
    "        self.X = torch.tensor(df[features].values, dtype=dtype, device=device)\n",
    "        self.y = torch.tensor(df[label].values, dtype=dtype, device=device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813e6662",
   "metadata": {},
   "source": [
    "Now we load the data again, clean it, and split it into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd42b2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "df_clean = clean_data(df)\n",
    "df_train, df_val = train_test_split(df_clean, test_size=0.2, random_state=42, stratify=df['Survived'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c382fe",
   "metadata": {},
   "source": [
    "These are the features we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4acf1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 'Survived'\n",
    "features = [feature for feature in df_clean.columns.tolist() if feature != label]\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f999e7",
   "metadata": {},
   "source": [
    "## Defining the Neural Network\n",
    "\n",
    "We have a 32-dimensional input vector, so the neural network will start with 33 inputs. We will treat this as a binary classification problem where we predict 1 (survived) or 0 (died).\n",
    "\n",
    "We start with a small neural network with three blocks consisting of linear -> batch norm -> GELU -> dropout, and one final linear layer to project back to 1-dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e9b02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    collections.OrderedDict([\n",
    "        ('lin1', nn.Linear(32, 48)),\n",
    "        ('norm1', nn.BatchNorm1d(48)),\n",
    "        ('gelu1', nn.GELU()),\n",
    "        ('drop1', nn.Dropout(0.5)),\n",
    "\n",
    "        ('lin2', nn.Linear(48, 64)),\n",
    "        ('norm2', nn.BatchNorm1d(64)),\n",
    "        ('gelu2', nn.GELU()),\n",
    "        ('drop2', nn.Dropout(0.5)),\n",
    "\n",
    "        ('lin3', nn.Linear(64, 96)),\n",
    "        ('norm3', nn.BatchNorm1d(96)),\n",
    "        ('gelu3', nn.GELU()),\n",
    "        ('drop3', nn.Dropout(0.5)),\n",
    "\n",
    "        ('lin4', nn.Linear(96, 1))\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b5e0fd",
   "metadata": {},
   "source": [
    "Hopefully we have a GPU so that training is faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a792a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb336df",
   "metadata": {},
   "source": [
    "We turn our training and validation datasets into data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3349ed28",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PandasDataset(df_train, features, label, device=device)\n",
    "val_dataset = PandasDataset(df_val, features, label, device=device)\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21495bc",
   "metadata": {},
   "source": [
    "We prepare for the training loop by moving the model to the device we're using (hopefully a GPU), define the optimizer used to update our weights and biases (AdamW), and specify that our loss function is binary cross entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992871c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9719dad",
   "metadata": {},
   "source": [
    "We also include a function to calculate the loss and accuracy over an entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3d6f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def loss_and_accuracy(split: str):\n",
    "    X: Tensor\n",
    "    y: Tensor\n",
    "    loss: Tensor\n",
    "    logits: Tensor\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total = 0\n",
    "    if split == 'train':\n",
    "        data_loader = train_loader\n",
    "    elif split == 'val':\n",
    "        data_loader = val_loader\n",
    "    else:\n",
    "        raise Exception(f'Invalid split {split}')\n",
    "    for X, y in data_loader:\n",
    "        logits = model(X)\n",
    "        loss = criterion(logits.squeeze(), y)\n",
    "        predictions = (torch.sigmoid(logits.squeeze()) >= 0.5).long()\n",
    "        total_correct += (predictions == y).sum().item()\n",
    "        total += y.shape[0]\n",
    "        total_loss += loss.item() * y.shape[0]\n",
    "    avg_loss = total_loss / total\n",
    "    accuracy = total_correct / total\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d774149e",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "This training loop trains the model with back-propagation and returns details about the training process (like loss and accuracy throughout the training run)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72529ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: nn.Module, epochs: int):\n",
    "    train_val_details = []\n",
    "    losses = []\n",
    "    learning_rates = []\n",
    "    for epoch in tqdm.tqdm(range(epochs)):\n",
    "        model.eval()\n",
    "        train_loss, train_accuracy = loss_and_accuracy('train')\n",
    "        val_loss, val_accuracy = loss_and_accuracy('val')\n",
    "        train_val_details.append((train_loss, train_accuracy, val_loss, val_accuracy))\n",
    "        learning_rates.append(scheduler.get_last_lr()[0])\n",
    "        model.train()\n",
    "        X: Tensor\n",
    "        y: Tensor\n",
    "        loss: Tensor\n",
    "        for X, y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(X)\n",
    "            loss = criterion(logits.squeeze(), y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "        scheduler.step()\n",
    "        epoch += 1\n",
    "        if epoch >= epochs:\n",
    "            break\n",
    "    df_train_val = pd.DataFrame(train_val_details, columns=['train loss', 'train accuracy', 'val loss', 'val accuracy'])\n",
    "    losses = np.array(losses)\n",
    "    learning_rates = np.array(learning_rates)\n",
    "    return df_train_val, losses, learning_rates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5cb145",
   "metadata": {},
   "source": [
    "We also define this plotting function to plot the details of the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30ae0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_plot(df_training_details: pd.DataFrame, losses: np.ndarray, learning_rates: np.ndarray, fig: Figure):\n",
    "    axes = []\n",
    "    gs = gridspec.GridSpec(3, 2, figure=fig)\n",
    "    axes.append(fig.add_subplot(gs[0, :]))\n",
    "    axes.append(fig.add_subplot(gs[1, 0]))\n",
    "    axes.append(fig.add_subplot(gs[1, 1]))\n",
    "    axes.append(fig.add_subplot(gs[2, 0]))\n",
    "    axes.append(fig.add_subplot(gs[2, 1]))\n",
    "\n",
    "    axes[0].scatter(np.arange(len(losses)), losses, alpha=0.5)\n",
    "    axes[0].set_title('Loss over training')\n",
    "    axes[0].set_xlabel('Iteration')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].grid(True)\n",
    "\n",
    "    axes[1].scatter(np.arange(df_training_details['train loss'].shape[0]), df_training_details['train loss'], label='training loss', alpha=0.5)\n",
    "    axes[1].scatter(np.arange(df_training_details['val loss'].shape[0]), df_training_details['val loss'], label='validation loss', alpha=0.5)\n",
    "    axes[1].set_title('Training and validation loss over training')\n",
    "    axes[1].set_xlabel('Iteration')\n",
    "    axes[1].set_ylabel('Loss')\n",
    "    axes[1].grid(True)\n",
    "    axes[1].legend()\n",
    "\n",
    "    axes[2].scatter(np.arange(len(learning_rates)), learning_rates, alpha=0.5)\n",
    "    axes[2].set_title('Learning rate over training')\n",
    "    axes[2].set_xlabel('Epoch')\n",
    "    axes[2].set_ylabel('Learning rate')\n",
    "    axes[2].grid(True)\n",
    "\n",
    "    axes[3].scatter(np.arange(df_training_details['train accuracy'].shape[0]), df_training_details['train accuracy'], label='training accuracy', alpha=0.5)\n",
    "    axes[3].scatter(np.arange(df_training_details['val accuracy'].shape[0]), df_training_details['val accuracy'], label='validation accuracy', alpha=0.5)\n",
    "    axes[3].set_title('Training and validation accuracy over training')\n",
    "    axes[3].set_xlabel('Iteration')\n",
    "    axes[3].set_ylabel('Accuracy')\n",
    "    axes[3].grid(True)\n",
    "    axes[3].legend()\n",
    "\n",
    "    axes[4].scatter(np.arange(df_training_details['train accuracy'].shape[0]), df_training_details['train accuracy'], label='training accuracy', alpha=0.5)\n",
    "    axes[4].scatter(np.arange(df_training_details['val accuracy'].shape[0]), df_training_details['val accuracy'], label='validation accuracy', alpha=0.5)\n",
    "    axes[4].set_title('Training and validation accuracy over training with y-axis from 0 to 1')\n",
    "    axes[4].set_xlabel('Iteration'); axes[3].set_ylabel('Accuracy')\n",
    "    axes[4].grid(True)\n",
    "    axes[4].legend()\n",
    "    axes[4].set_ylim([0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa109bc",
   "metadata": {},
   "source": [
    "Here we train the model for 15 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e249d449",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training_details, losses, learning_rates = train(model, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e478313",
   "metadata": {},
   "source": [
    "The graphs below show the loss and accuracy throughout the 15 epochs, as well as the learning rate as it decays. Note that the model seems to peak around 80% accuracy on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d8d9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig: Figure\n",
    "axes: PltAxes\n",
    "fig = plt.figure(figsize=(15, 15))\n",
    "training_plot(df_training_details, losses, learning_rates, fig)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa273b54",
   "metadata": {},
   "source": [
    "### Using a Larger Neural Network\n",
    "\n",
    "The larger model has seven blocks of linear -> batch norm -> GELU -> dropout, then a final linear layer. The largest layer in this model has 512 neurons, up from 96 neurons in the largest layer in the smaller model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060ce33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_model = nn.Sequential(\n",
    "    collections.OrderedDict([\n",
    "        ('lin1', nn.Linear(32, 48)),\n",
    "        ('norm1', nn.BatchNorm1d(48)),\n",
    "        ('gelu1', nn.GELU()),\n",
    "        ('drop1', nn.Dropout(0.5)),\n",
    "\n",
    "        ('lin2', nn.Linear(48, 64)),\n",
    "        ('norm2', nn.BatchNorm1d(64)),\n",
    "        ('gelu2', nn.GELU()),\n",
    "        ('drop2', nn.Dropout(0.5)),\n",
    "\n",
    "        ('lin3', nn.Linear(64, 96)),\n",
    "        ('norm3', nn.BatchNorm1d(96)),\n",
    "        ('gelu3', nn.GELU()),\n",
    "        ('drop3', nn.Dropout(0.5)),\n",
    "\n",
    "        ('lin4', nn.Linear(96, 128)),\n",
    "        ('norm4', nn.BatchNorm1d(128)),\n",
    "        ('gelu4', nn.GELU()),\n",
    "        ('drop4', nn.Dropout(0.5)),\n",
    "\n",
    "        ('lin5', nn.Linear(128, 256)),\n",
    "        ('norm5', nn.BatchNorm1d(256)),\n",
    "        ('gelu5', nn.GELU()),\n",
    "        ('drop5', nn.Dropout(0.5)),\n",
    "\n",
    "        ('lin6', nn.Linear(256, 512)),\n",
    "        ('norm6', nn.BatchNorm1d(512)),\n",
    "        ('gelu6', nn.GELU()),\n",
    "        ('drop6', nn.Dropout(0.5)),\n",
    "\n",
    "        ('lin7', nn.Linear(512, 128)),\n",
    "        ('norm7', nn.BatchNorm1d(128)),\n",
    "        ('gelu7', nn.GELU()),\n",
    "        ('drop7', nn.Dropout(0.5)),\n",
    "\n",
    "        ('lin8', nn.Linear(128, 1))\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77af01fd",
   "metadata": {},
   "source": [
    "To better accommodate the larger network, we set the learning rate scheduler decay rate to 0.99 so that we can train for more epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fc3b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_model.to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(big_model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74103361",
   "metadata": {},
   "source": [
    "Here we train the larger network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd71628f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training_details, losses, learning_rates = train(big_model, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c03644",
   "metadata": {},
   "source": [
    "Note that the larger model performs almost identically to the small model, indicating that to achieve better performance we would need to engineer better features than what we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f2fa52",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig: Figure\n",
    "axes: PltAxes\n",
    "fig = plt.figure(figsize=(15, 15))\n",
    "training_plot(df_training_details, losses, learning_rates, fig)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0955304a",
   "metadata": {},
   "source": [
    "## Saving the Model and Generating the Competition Submission\n",
    "\n",
    "We will save the smaller model (and use it for prediction) since it's performance is on par with the large model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a0ad60",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './model/model.pth')\n",
    "print('Model saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4084ff14",
   "metadata": {},
   "source": [
    "We load the competition dataset and clean it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed2a002",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission = pd.read_csv('test.csv')\n",
    "df_submission = clean_data(df_submission, keep_id=True)\n",
    "df_submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66f81fb",
   "metadata": {},
   "source": [
    "Initially, we predict all passengers to have died, then we change it to survived on an individual basis according to the model's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eacdfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_features = list(df_submission.columns)\n",
    "submission_features.remove('PassengerId')\n",
    "df_submission['Survived'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec48854",
   "metadata": {},
   "source": [
    "Here is where we run inference and find passengers that are likely to survive (according to the model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5ccb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for idx, row in df_submission.iterrows():\n",
    "        X = torch.tensor(row[submission_features].values, dtype=torch.float32, device=device)\n",
    "        logits = model(X.unsqueeze(0))\n",
    "        predictions = (torch.sigmoid(logits.squeeze()) >= 0.5).long()\n",
    "        if predictions.item() == 1:\n",
    "            df_submission.loc[idx, 'Survived'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5100ad3",
   "metadata": {},
   "source": [
    "Export only the ID and survived columns to CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668c5a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission = df_submission[['PassengerId', 'Survived']]\n",
    "df_submission.to_csv('./submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_cuda12.2",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
